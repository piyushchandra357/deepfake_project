{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Deepfake Detection"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-01-13T07:35:31.059238Z","iopub.status.busy":"2022-01-13T07:35:31.058881Z","iopub.status.idle":"2022-01-13T07:35:31.429999Z","shell.execute_reply":"2022-01-13T07:35:31.429086Z","shell.execute_reply.started":"2022-01-13T07:35:31.059185Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from skimage.color import rgb2gray\n","import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from scipy import ndimage\n","import os\n","import sys\n","import random\n","import math\n","import numpy as np\n","import skimage.io\n","import matplotlib\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Review of Data Files"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:35:31.434130Z","iopub.status.busy":"2022-01-13T07:35:31.433866Z","iopub.status.idle":"2022-01-13T07:35:31.842094Z","shell.execute_reply":"2022-01-13T07:35:31.841402Z","shell.execute_reply.started":"2022-01-13T07:35:31.434081Z"},"trusted":true},"outputs":[],"source":["train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n","train_sample_metadata.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:35:31.844071Z","iopub.status.busy":"2022-01-13T07:35:31.843750Z","iopub.status.idle":"2022-01-13T07:35:32.090052Z","shell.execute_reply":"2022-01-13T07:35:32.089257Z","shell.execute_reply.started":"2022-01-13T07:35:31.844022Z"},"trusted":true},"outputs":[],"source":["train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Recognizing people in a video stream"]},{"cell_type":"markdown","metadata":{},"source":["## From video to frames\n","In **cv2.VideoCapture(VIDEO_STREAM)**, we just have to mention the video name with itâ€™s extension. \n","\n","You can set frame rate which is widely known as fps (frames per second). Here I set 0.5 so it will capture a frame at every 0.5 seconds, means 2 frames (images) for each second.\n","\n","It will save images with name as **image1.jpg**, **image2.jpg** and so on."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:35:32.092249Z","iopub.status.busy":"2022-01-13T07:35:32.091613Z","iopub.status.idle":"2022-01-13T07:35:50.094847Z","shell.execute_reply":"2022-01-13T07:35:50.093992Z","shell.execute_reply.started":"2022-01-13T07:35:32.092188Z"},"trusted":true},"outputs":[],"source":["import cv2\n","\n","VIDEO_STREAM = \"/kaggle/input/deepfake-detection-challenge/test_videos/ytddugrwph.mp4\"\n","#VIDEO_STREAM_OUT = \"/kaggle/input/deepfake-detection-challenge/test_videos/Result.mp4\"\n","\n","vidcap = cv2.VideoCapture(VIDEO_STREAM)\n","def getFrame(sec):\n","    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n","    hasFrames,image = vidcap.read()\n","    if hasFrames:\n","        cv2.imwrite(\"image\"+str(count)+\".jpg\", image) # save frame as JPG file\n","        plt.imshow(image)\n","    return hasFrames\n","sec = 0\n","frameRate = 0.5 #//it will capture image in each 0.5 second\n","count=1\n","success = getFrame(sec)\n","while success:\n","    count = count + 1\n","    sec = sec + frameRate\n","    sec = round(sec, 2)\n","    success = getFrame(sec)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Face Detection\n","\n","Source: https://www.kaggle.com/robikscube/kaggle-deepfake-detection-introduction\n"]},{"cell_type":"markdown","metadata":{},"source":["## Locating a face within an image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:35:50.098951Z","iopub.status.busy":"2022-01-13T07:35:50.098656Z","iopub.status.idle":"2022-01-13T07:46:51.324902Z","shell.execute_reply":"2022-01-13T07:46:51.324132Z","shell.execute_reply.started":"2022-01-13T07:35:50.098897Z"},"trusted":true},"outputs":[],"source":["!pip install face_recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:46:51.327159Z","iopub.status.busy":"2022-01-13T07:46:51.326855Z","iopub.status.idle":"2022-01-13T07:46:56.260195Z","shell.execute_reply":"2022-01-13T07:46:56.259347Z","shell.execute_reply.started":"2022-01-13T07:46:51.327086Z"},"trusted":true},"outputs":[],"source":["import face_recognition\n","import cv2 as cv\n","import os\n","import matplotlib.pylab as plt\n","from PIL import Image\n","train_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\n","train_video_files = [train_dir + x for x in os.listdir(train_dir)]\n","# video_file = train_video_files[30]\n","video_file = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/afoovlsmtx.mp4'\n","cap = cv.VideoCapture(video_file)\n","success, image = cap.read()\n","image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","cap.release() \n","face_locations = face_recognition.face_locations(image)\n","\n","# https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py\n","print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n","\n","for face_location in face_locations:\n","\n","    # Print the location of each face in this image\n","    top, right, bottom, left = face_location\n","    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n","    # Access the actual face itself:\n","    face_image = image[top:bottom, left:right]\n","    fig, ax = plt.subplots(1,1, figsize=(5, 5))\n","    plt.grid(False)\n","    ax.xaxis.set_visible(False)\n","    ax.yaxis.set_visible(False)\n","    ax.imshow(face_image)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Display test examples and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:46:56.262312Z","iopub.status.busy":"2022-01-13T07:46:56.261766Z","iopub.status.idle":"2022-01-13T07:47:31.394635Z","shell.execute_reply":"2022-01-13T07:47:31.392325Z","shell.execute_reply.started":"2022-01-13T07:46:56.262260Z"},"trusted":true},"outputs":[],"source":["from PIL import Image, ImageDraw\n","\n","fig, axs = plt.subplots(19, 2, figsize=(10, 80))\n","axs = np.array(axs)\n","axs = axs.reshape(-1)\n","i = 0\n","pad = 60 #pad is addded to the plot to zoom out of the face\n","for fn in train_sample_metadata.index[:24]:\n","    label = train_sample_metadata.loc[fn]['label']\n","    orig = train_sample_metadata.loc[fn]['label']\n","    video_file = f'/kaggle/input/deepfake-detection-challenge/train_sample_videos/{fn}'\n","    ax = axs[i]\n","    cap = cv.VideoCapture(video_file)\n","    success, image = cap.read()\n","    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","    face_locations = face_recognition.face_locations(image)\n","    if len(face_locations) > 0:\n","        # Print first face\n","        face_location = face_locations[0]\n","        top, right, bottom, left = face_location\n","        face_image = image[top-pad:bottom+pad, left-pad:right+pad]\n","        ax.imshow(face_image)\n","        ax.grid(False)\n","        ax.title.set_text(f'{fn} - {label}')\n","        ax.xaxis.set_visible(False)\n","        ax.yaxis.set_visible(False)\n","        # Find landmarks\n","        face_landmarks_list = face_recognition.face_landmarks(face_image)\n","        if len(face_landmarks_list) > 0:\n","            face_landmarks = face_landmarks_list[0]\n","            pil_image = Image.fromarray(face_image)\n","            d = ImageDraw.Draw(pil_image)\n","            for facial_feature in face_landmarks.keys():\n","                d.line(face_landmarks[facial_feature], width=2, fill='yellow')\n","            landmark_face_array = np.array(pil_image)\n","            ax2 = axs[i+1]\n","            ax2.imshow(landmark_face_array)\n","            ax2.grid(False)\n","            ax2.title.set_text(f'{fn} - {label}')\n","            ax2.xaxis.set_visible(False)\n","            ax2.yaxis.set_visible(False)\n","            i += 2\n","plt.grid(False)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Baseline submission using Facenet\n","\n","#### Baseline code is taked from this kernel: https://www.kaggle.com/climbest/facial-recognition-model-in-pytorch-change-bias"]},{"cell_type":"markdown","metadata":{},"source":["### Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:47:31.396403Z","iopub.status.busy":"2022-01-13T07:47:31.395934Z","iopub.status.idle":"2022-01-13T07:47:38.835105Z","shell.execute_reply":"2022-01-13T07:47:38.834026Z","shell.execute_reply.started":"2022-01-13T07:47:31.396357Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# Install facenet-pytorch\n","%pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-1.0.1-py3-none-any.whl\n","\n","# Copy model checkpoints to torch cache so they are loaded automatically by the package\n","!mkdir -p /tmp/.cache/torch/checkpoints/\n","!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth /tmp/.cache/torch/checkpoints/vggface2_DG3kwML46X.pt\n","!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth /tmp/.cache/torch/checkpoints/vggface2_G5aNV2VSMn.pt"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:47:38.838859Z","iopub.status.busy":"2022-01-13T07:47:38.838635Z","iopub.status.idle":"2022-01-13T07:47:38.851172Z","shell.execute_reply":"2022-01-13T07:47:38.850368Z","shell.execute_reply.started":"2022-01-13T07:47:38.838827Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import torch\n","import cv2\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","# See github.com/timesler/facenet-pytorch:\n","from facenet_pytorch import MTCNN, InceptionResnetV1\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu' #checks if GPU is being used or the CPU\n","print(f'Running on device: {device}')"]},{"cell_type":"markdown","metadata":{},"source":["## Create MTCNN and Inception Resnet models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:47:38.853178Z","iopub.status.busy":"2022-01-13T07:47:38.852671Z","iopub.status.idle":"2022-01-13T07:47:53.307008Z","shell.execute_reply":"2022-01-13T07:47:53.306213Z","shell.execute_reply.started":"2022-01-13T07:47:38.852975Z"},"trusted":true},"outputs":[],"source":["# Load face detector\n","mtcnn = MTCNN(device=device).eval()\n","\n","# Load facial recognition model\n","resnet = InceptionResnetV1(pretrained='vggface2', num_classes=2, device=device).eval()"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Process test videos"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T07:47:53.308704Z","iopub.status.busy":"2022-01-13T07:47:53.308404Z","iopub.status.idle":"2022-01-13T08:15:50.549976Z","shell.execute_reply":"2022-01-13T08:15:50.549169Z","shell.execute_reply.started":"2022-01-13T07:47:53.308659Z"},"trusted":true},"outputs":[],"source":["# Get all test videos\n","filenames = glob.glob('/kaggle/input/deepfake-detection-challenge/test_videos/*.mp4')\n","\n","# Number of frames to sample (evenly spaced) from each video\n","n_frames = 10\n","\n","X = []\n","with torch.no_grad():\n","    for i, filename in enumerate(filenames):\n","        print(f'Processing {i+1:5n} of {len(filenames):5n} videos\\r', end='')\n","        \n","        try:\n","            # Create video reader and find length\n","            v_cap = cv2.VideoCapture(filename)\n","            v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","            \n","            # Pick 'n_frames' evenly spaced frames to sample\n","            sample = np.linspace(0, v_len - 1, n_frames).round().astype(int)\n","            imgs = []\n","            for j in range(v_len):\n","                success, vframe = v_cap.read()\n","                vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2RGB)\n","                if j in sample:\n","                    imgs.append(Image.fromarray(vframe))\n","            v_cap.release()\n","            \n","            # Pass image batch to MTCNN as a list of PIL images\n","            faces = mtcnn(imgs)\n","            \n","            # Filter out frames without faces\n","            faces = [f for f in faces if f is not None]\n","            faces = torch.stack(faces).to(device)\n","            \n","            # Generate facial feature vectors using a pretrained model\n","            embeddings = resnet(faces)\n","            \n","            # Calculate centroid for video and distance of each face's feature vector from centroid\n","            centroid = embeddings.mean(dim=0)\n","            X.append((embeddings - centroid).norm(dim=1).cpu().numpy())\n","        except KeyboardInterrupt:\n","            raise Exception(\"Stopped.\")\n","        except:\n","            X.append(None)"]},{"cell_type":"markdown","metadata":{},"source":["# 8. Predict classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T08:15:50.551660Z","iopub.status.busy":"2022-01-13T08:15:50.551360Z","iopub.status.idle":"2022-01-13T08:15:50.565451Z","shell.execute_reply":"2022-01-13T08:15:50.564248Z","shell.execute_reply.started":"2022-01-13T08:15:50.551615Z"},"trusted":true},"outputs":[],"source":["bias = -0.4\n","weight = 0.068235746\n","\n","submission = []\n","for filename, x_i in zip(filenames, X):\n","    if x_i is not None and len(x_i) == 10:\n","        prob = 1 / (1 + np.exp(-(bias + (weight * x_i).sum())))\n","    else:\n","        prob = 0.6\n","    submission.append([os.path.basename(filename), prob])"]},{"cell_type":"markdown","metadata":{},"source":["# 9. Output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T08:15:50.568054Z","iopub.status.busy":"2022-01-13T08:15:50.566707Z","iopub.status.idle":"2022-01-13T08:15:50.586337Z","shell.execute_reply":"2022-01-13T08:15:50.585425Z","shell.execute_reply.started":"2022-01-13T08:15:50.567075Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(submission, columns=['filename', 'label'])\n","submission.sort_values('filename').to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T08:15:50.588696Z","iopub.status.busy":"2022-01-13T08:15:50.588097Z","iopub.status.idle":"2022-01-13T08:15:50.852124Z","shell.execute_reply":"2022-01-13T08:15:50.851409Z","shell.execute_reply.started":"2022-01-13T08:15:50.588563Z"},"trusted":true},"outputs":[],"source":["plt.hist(submission.label, 20)\n","plt.show()\n","submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-13T08:15:50.853864Z","iopub.status.busy":"2022-01-13T08:15:50.853402Z","iopub.status.idle":"2022-01-13T08:15:50.857256Z","shell.execute_reply":"2022-01-13T08:15:50.856546Z","shell.execute_reply.started":"2022-01-13T08:15:50.853817Z"},"trusted":true},"outputs":[],"source":["# to be continued...."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
